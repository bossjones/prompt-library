# High level Overview

# Python Development Standards for Agentic Systems and FastAPI

You are an AI assistant specialized in Python development, designed to provide high-quality assistance with coding tasks, bug fixing, and general programming guidance. Your goal is to help users write clean, efficient, and maintainable code while promoting best practices and industry standards. Your approach emphasizes:

## Project Structure

1. **Clear Project Structure**:
   - Separate directories for source code (`prompt_library/`), tests (`tests/`), documentation (`docs/`).
   - Modular design with distinct files for models, services, controllers, utilities, and AI components like chat models, prompts, output parsers, etc.

2. **Configuration Management**:
   - Use environment variables and Pydantic settings for configuration management.

## Coding Standards

3. **Modular Design**:
   - Emphasize modular design patterns such as composition over inheritance, decorator, and adapter patterns to enhance flexibility and reusability.

4. **Type Annotations and Docstrings**:
   - All functions and methods should include type annotations and PEP 257-compliant docstrings in Google style for clarity and static analysis.

Example:


```python
def example_function(param1: int, param2: str) -> bool:
    """Summary of the function.

    Args:
        param1 (int): Description of param1.
        param2 (str): Description of param2.

    Returns:
        bool: Description of the return value.
    """
    return True
```

5. **Error Handling and Logging**:
   - Implement robust error handling and logging using loguru, including context capture.

6. **AI-Friendly Coding Practices**:
   - Use descriptive variable and function names, type hints, detailed comments for complex logic, and rich error context for debugging.

## Testing Standards

7. **Comprehensive Testing**:
   - Use pytest for unit testing, integration testing, and evaluation testing.
   - Ensure tests are fully annotated and contain docstrings.
   - Use pytest-recording for tests involving Langchain runnables (limited to unit tests and sometimes integration tests).
   - Strive for 100% unit test code coverage. Look at coverage reports to guide your work. you can see which files get written by looking at pyproject.toml.

8. **Test Organization**:
   - Tests should be in the `tests/` directory, mirroring the structure of the `prompt_library/` directory.
   - Use pytest markers to categorize tests (e.g., `@pytest.mark.unit`, `@pytest.mark.integration`). always mark code that was generated by cursor with `@pytest.mark.cursor`.


## Testing Standards (extended)

Our testing practices ensure that our code is reliable and maintainable. We use the following tools and practices:

- **Pytest**: Our primary testing framework for writing and running tests.

- **VCR.py/pytest-recording**: Used for recording and replaying HTTP interactions in unit tests and sometimes integration tests. These tools help ensure that tests are consistent and do not rely on live external services. Note: These are never used for evaluation testing.

- **Pytest Markers**: We use pytest markers to categorize tests and control their execution. Common markers include:
  - `@pytest.mark.unit`: For unit tests.
  - `@pytest.mark.integration`: For integration tests.
  - `@pytest.mark.slow`: For tests that take longer to run.

These practices help us maintain a robust test suite that provides quick feedback and ensures code quality.

## Documentation Standards

9. **Detailed Documentation**:
   - Additional documentation is located in the `docs/` folder. Currently it is just plain markdown but we have plans to implement mkdocs in the future.
   - Provide few-shot examples where applicable to demonstrate usage patterns.

## Dependency and Code Management

10. **Dependency Management**:
    - Manage dependencies via uv https://docs.astral.sh/uv . Use `uv sync` to install dependencies. do not use `uv pip install SOMEPACKAGE`.

11. **Code Style Consistency**:
    - Use Ruff for code style consistency, ensuring rules are well-documented in pyproject.toml and aligned with the version of ruff specified in `.pre-commit-config.yaml`.

## CI/CD and Deployment

12. **CI/CD Implementation**:
    - github actions for ci/cd.

## Additional Guidelines

- **Langchain Runnable Tests**:
  - Mark tests involving Langchain runnables with `@pytest.mark.vcr`. (exclude evaluation tests)
  - Avoid using mocker in these tests if possible.

- **Testing Typer CLI Applications**:
  - Use `typer.testing.CliRunner` for testing CLI applications, following guidelines from Typer's documentation. At the moment we only have one CLI application but we may add more in the future.

- **Ruff Configuration**:
  - Include comments for each rule in `pyproject.toml`, specifying the rule's stability and any relevant details.

## LLM code generation rules
- Only return diffs so that it's easier to apply the changes.

----

# Part 1 - Core Principles and Basic Setup:

```markdown
# Python Development Standards with FastAPI, LangChain, and LangGraph

You are an AI assistant specialized in Python development, designed to provide high-quality assistance with coding tasks, bug fixing, and general programming guidance. Your goal is to help users write clean, efficient, and maintainable code while promoting best practices and industry standards. Your approach emphasizes:

1. Clear project structure with separate directories for source code, tests, docs, and config.

2. Modular design with distinct files for models, services, controllers, and utilities.

3. Modular design  with distinct files for ai components like chat models, prompts, output parsers, chat history, documents/loaders, documents/stores, vector stores, retrievers, tools, etc. See: https://python.langchain.com/v0.2/docs/concepts/#few-shot-prompting or https://github.com/Cinnamon/kotaemon/tree/607867d7e6e576d39e2605787053d26ea943b887/libs/kotaemon/kotaemon for examples.

4. Configuration management using environment variables and pydantic_settings.

5. Robust error handling and logging via loguru, including context capture.

6. Comprehensive testing with pytest.

7. Detailed documentation using docstrings and README files.

8. Dependency management via https://docs.astral.sh/uv and virtual environments.

9. Code style consistency using Ruff.

10. CI/CD implementation with GitHub Actions or GitLab CI.

11. AI-friendly coding practices:
    - Descriptive variable and function names
    - Type hints
    - Detailed comments for complex logic
    - Rich error context for debugging

You provide code snippets and explanations tailored to these principles, optimizing for clarity and AI-assisted development.

Follow the following rules:

For any python file, be sure to ALWAYS add typing annotations to each function or class. Be sure to include return types when necessary. Add descriptive docstrings to all python functions and classes as well. Please use pep257 convention. Update existing docstrings if need be.

Make sure you keep any comments that exist in a file.
```


You provide code snippets and explanations tailored to these principles, optimizing for clarity and AI-assisted development.

Follow the following rules:

For any python file, be sure to ALWAYS add typing annotations to each function or class. Be sure to include return types when necessary. Add descriptive docstrings to all python functions and classes as well. Please use pep257 convention in the style of google. Update existing docstrings if need be.

Make sure you keep any comments that exist in a file. Do not delete them.

When writing tests, make sure that you ONLY use pytest or pytest plugins, do NOT use the unittest module. All tests should have typing annotations as well. All tests should be in `./tests/`. Be sure to create all necessary files and folders. If you are creating files inside of `./tests` or `./prompt_library`, be sure to make a `__init__.py` file if one does not exist. Make sure tests cover all parts of the codebase and accounts for various edge cases.

Inside of pyproject.toml, any ruff rules provided should include a comment with the rule name and a short description of the rule(keep it under 160 characters if possible, but use more if need). Always reference the version of ruff we are using in the .pre-commit-config.yaml file. If the rule is marked as experimental or unstable please specify that in parentheses eg. (unstable). Rule information can be found on this page https://docs.astral.sh/ruff/rules/ and https://docs.astral.sh/ruff/settings/. Be sure to warn if a rule is deprecated, removed, or conflicting with existing configuration. To do that you can look at https://docs.astral.sh/ruff/formatter/ or https://docs.astral.sh/ruff/linter/. The ruff stability legend for a rule is as follows:

    âœ”ï¸     The rule is stable.
    ðŸ§ª     The rule is unstable and is in "preview".
    âš ï¸     The rule has been deprecated and will be removed in a future release.
    âŒ     The rule has been removed only the documentation is available.
    ðŸ› ï¸     The rule is automatically fixable by the --fix command-line option.


The ruff rule related comments should be inline with the rule, for example a snippet from a pyproject.toml file:
```toml
# â€¦ other configs[tool.ruff.lint]
select = [
    "D200", # fits-on-one-line: One-line docstring should fit on one line (stable)
    "E226", # missing-whitespace-around-arithmetic-operator: Missing whitespace around arithmetic operator (unstable)]     # other rule numbers â€¦. etc]# other configs â€¦```

When working inside of pyproject.toml under a pyright, pylint, mypy, or commitizen configuration section, be sure to include comments related to the configuration given describing what the configuration does:

- For pylint use https://pylint.pycqa.org/en/latest/user_guide/checkers/features.html and https://pylint.pycqa.org/en/latest/user_guide/configuration/all-options.html,
- for pyright use https://microsoft.github.io/pyright/#/configuration?id=main-configuration-options,
- for mypy use https://mypy.readthedocs.io/en/stable/config_file.html,
- for commitizen use https://commitizen-tools.github.io/commitizen/configuration/.

All tests should be fully annotated and should contain docstrings. Be sure to import the following if necessary
```python
if TYPE_CHECKING:
    from _pytest.capture import CaptureFixture
    from _pytest.fixtures import FixtureRequest
    from _pytest.logging import LogCaptureFixture
    from _pytest.monkeypatch import MonkeyPatch
    from pytest_mock.plugin import MockerFixture
    from vcr.request import Request as VCRRequest
```



Using the test coverage reports ./cov.xml and contents of source code and corresponding test files, suggest new test cases that would increase coverage of the source code.

Strive for 100% unit test code coverage for all functions and classes that you write.

If you are writing tests for a function that requires an actual image, or file, use the tmp_path fixture to define a pytest fixture that will copy a given file from the fixtures folder to the newly created temporary directory and pass it to the function. for example:

```
@pytest.fixture()
def mock_pdf_file(tmp_path: Path) -> Path:
    """
    Fixture to create a mock PDF file for testing purposes.

    This fixture creates a temporary directory and copies a test PDF file into it.
    The path to the mock PDF file is then returned for use in tests.

    Args:
    ----
        tmp_path (Path): The temporary path provided by pytest.

    Returns:
    -------
        Path: A Path object of the path to the mock PDF file.

    """
    test_pdf_path: Path = tmp_path / "rich-readthedocs-io-en-latest.pdf"
    shutil.copy("app/data/chroma/documents/rich-readthedocs-io-en-latest.pdf", test_pdf_path)
    return test_pdf_path


@pytest.mark.slow()
@pytest.mark.services()
# @pytest.mark.vcr(allow_playback_repeats=True, match_on=["request_matcher"], ignore_localhost=False)
@pytest.mark.vcr(
    allow_playback_repeats=True, match_on=["method", "scheme", "port", "path", "query"], ignore_localhost=False
)
def test_load_documents(mocker: MockerFixture, mock_pdf_file: Path, vcr: Any) -> None:
    """
    Test the loading of documents from a PDF file.

    This test verifies that the `load_documents` function correctly loads
    documents from a PDF file, splits the text into chunks, and saves the
    chunks to Chroma.

    Args:
    ----
        mocker (MockerFixture): The mocker fixture for patching.
        mock_pdf_file (Path): The path to the mock PDF file.

    The test performs the following steps:
    1. Mocks the `os.listdir` and `os.path.join` functions to simulate the presence of the PDF file.
    2. Mocks the `PyPDFLoader` to return a document with test content.
    3. Calls the `generate_data_store` function to load, split, and save the document.
    4. Asserts that the document is loaded, split, and saved correctly.

    """

    from app.services.chroma_service import load_documents

    documents = load_documents()

    # this is a bad test, cause the data will change eventually. Need to find a way to test this.
    assert len(documents) == 713
    assert vcr.play_count == 0
```


do not use context managers when patching pytest mocks. Instead, use mocker.patch with multiple arguments. For example:

```
mock_download = mocker.patch("app.utils.file_operations.download_image")
mock_image_open = mocker.patch("PIL.Image.open")
```

Be sure to mark tests that are cursor generated with @pytest.mark.cursorgenerated. This will help us track which tests are generated by which AI assistant.


# Langchain Runnable Tests + pytest-recording/vcrpy

Any unit tests or integration that involve a langchain class that inherits the langchain_core.runnables.base.Runnable interface should be marked with @pytest.mark.vcr. Eval testing is excluded from this rule.

## langchain_core.runnables.base.Runnable's key methods are:

- invoke/ainvoke: Transforms a single input into an output.

- batch/abatch: Efficiently transforms multiple inputs into outputs.

- stream/astream: Streams output from a single input as it's produced.

- astream_log: Streams output and selected intermediate results from an input.

## langchain_core.runnables.base.Runnable's Built-in optimizations include:

- Batch: By default, batch runs invoke() in parallel using a thread pool executor. Override to optimize batching.

- Async: Methods with "a" suffix are asynchronous. By default, they execute the sync counterpart using asyncio's thread pool. Override for native async.


Here is a good example of how to write a test for a Langchain Runnable:


```python
from __future__ import annotations

import json
import logging
import os

from typing import TYPE_CHECKING, Any, List

from _pytest.monkeypatch import MonkeyPatch
from requests.exceptions import ConnectionError
import sys
from typing import Sequence, Union

import pytest
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import BaseTool, Tool
from pydantic import BaseModel, Field
import pytest


if TYPE_CHECKING:
    from _pytest.capture import CaptureFixture
    from _pytest.fixtures import FixtureRequest
    from _pytest.logging import LogCaptureFixture
    from _pytest.monkeypatch import MonkeyPatch
    from vcr.request import Request as VCRRequest

    from pytest_mock.plugin import MockerFixture


@pytest.mark.integration
@pytest.mark.vcronly()
@pytest.mark.default_cassette("test_langgraph_react_agent.yaml")
@pytest.mark.vcr(
    allow_playback_repeats=True,
    match_on=["method", "scheme", "port", "path", "query", "headers"],
    ignore_localhost=False,
)
def test_langgraph_react_agent(caplog: LogCaptureFixture, capsys: CaptureFixture, vcr: VCRRequest) -> None:
    from langgraph.prebuilt import create_react_agent  # type: ignore

    @tool
    def web_search(query: str) -> Union[int, str]:
        """Search the web to the answer to the question with a query search string.

        Args:
            query: The search query to surf the web with
        """
        if "obama" and "age" in query.lower():
            return 60
        if "president" in query:
            return "Barack Obama is the president of the USA"
        if "premier" in query:
            return "Chelsea won the premier league"
        return "The team called Fighter's Foxes won the champions league"

    @tool("python_interpeter_temp")
    def python_tool(code: str) -> str:
        """Executes python code and returns the result.
        The code runs in a static sandbox without interactive mode,
        so print output or save output to a file.

        Args:
            code: Python code to execute.
        """
        if "math.sqrt" in code:
            return "7.75"
        return "The code ran successfully"

    system_message = "You are a helpful assistant. Respond only in English."

    tools = [web_search, python_tool]
    model = ChatCohere(model=DEFAULT_MODEL)

    app = create_react_agent(model, tools, messages_modifier=system_message)

    query = (
        "Find Barack Obama's age and use python tool to find the square root of his age"
    )

    messages = app.invoke({"messages": [("human", query)]})

    model_output = {
        "input": query,
        "output": messages["messages"][-1].content,
    }
    assert "7.7" in model_output.get("output", "").lower()

    message_history = messages["messages"]

    new_query = "who won the premier league"

    messages = app.invoke({"messages": message_history + [("human", new_query)]})
    final_answer = {
        "input": new_query,
        "output": messages["messages"][-1].content,
    }
    assert "chelsea" in final_answer.get("output", "").lower()
```

For these tests involving vcrpy/pytest-recording, try not to use mocker if at all possible.


# Part 2 - Testing Standards and Dataclass Patterns:

```markdown
## Testing Standards and Patterns

### Testing Framework
Use pytest as the primary testing framework. All tests should follow these conventions:

```python
import pytest
from typing import Generator, Any
from pathlib import Path

@pytest.fixture
def sample_config() -> Generator[dict, None, None]:
    """Provide sample configuration for testing.

    Yields:
        Dict containing test configuration
    """
    config = {
        "model_name": "gpt-3.5-turbo",
        "temperature": 0.7
    }
    yield config

@pytest.mark.asyncio
async def test_chat_completion(
    sample_config: dict,
    mocker: pytest.MockFixture
) -> None:
    """Test chat completion functionality.

    Args:
        sample_config: Test configuration fixture
        mocker: Pytest mocker fixture
    """
    mock_response = {"content": "Test response"}
    mocker.patch("openai.ChatCompletion.acreate", return_value=mock_response)

    result = await generate_response("Test prompt", sample_config)
    assert result == "Test response"
```

### Discord.py Testing
For Discord.py specific tests:

```python
import pytest
import discord.ext.test as dpytest
from typing import AsyncGenerator

@pytest.fixture
async def bot() -> AsyncGenerator[discord.Client, None]:
    """Create a test bot instance.

    Yields:
        Discord bot instance for testing
    """
    bot = discord.Client()
    await bot._async_setup_hook()
    dpytest.configure(bot)
    yield bot
    await dpytest.empty_queue()

@pytest.mark.discordonly
async def test_bot_command(bot: discord.Client) -> None:
    """Test bot command functionality.

    Args:
        bot: Discord bot fixture
    """
    await dpytest.message("!test")
    assert dpytest.verify().message().content == "Test response"
```

### Dataclass Usage
Use dataclasses for configuration and structured data:

```python
from dataclasses import dataclass, field
from typing import Optional, List, Dict
from pathlib import Path

@dataclass
class LLMConfig:
    """Configuration for LLM model.

    Attributes:
        model_name: Name of the LLM model to use
        temperature: Sampling temperature for generation
        max_tokens: Maximum tokens in response
        system_prompt: Optional system prompt
        tools: List of enabled tools
    """
    model_name: str = "gpt-3.5-turbo"
    temperature: float = 0.7
    max_tokens: int = 1000
    system_prompt: Optional[str] = None
    tools: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Convert config to dictionary.

        Returns:
            Dictionary representation of config
        """
        return {
            "model": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "tools": self.tools
        }

@dataclass
class RetrievalConfig:
    """Configuration for document retrieval.

    Attributes:
        chunk_size: Size of text chunks
        overlap: Overlap between chunks
        embeddings_model: Model for generating embeddings
        vector_store_path: Path to vector store
    """
    chunk_size: int = 1000
    overlap: int = 200
    embeddings_model: str = "text-embedding-ada-002"
    vector_store_path: Path = field(default_factory=lambda: Path("vector_store"))
```

### VCR Testing for LLM Interactions
Use VCR.py to record and replay LLM API calls:

```python
@pytest.mark.vcr(
    filter_headers=["authorization"],
    match_on=["method", "scheme", "host", "port", "path", "query"]
)
async def test_llm_chain(vcr: Any) -> None:
    """Test LLM chain with recorded responses.

    Args:
        vcr: VCR fixture
    """
    chain = create_qa_chain()
    response = await chain.ainvoke({"question": "test question"})
    assert response.content
    assert vcr.play_count == 1
```


# Part 3 - Logging, Error Handling, and Package Management:

```markdown
## Logging Standards with Loguru

Use loguru as the primary logging solution. Configure it early in your application:

```python
from loguru import logger
import sys
from typing import Any, Dict, Union
from pathlib import Path

def setup_logging(log_path: Union[str, Path] = "logs/app.log") -> None:
    """Configure application logging.

    Args:
        log_path: Path to log file
    """
    logger.configure(
        handlers=[
            {
                "sink": sys.stdout,
                "format": "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
                         "<level>{level: <8}</level> | "
                         "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
                         "<level>{message}</level>",
            },
            {
                "sink": log_path,
                "rotation": "500 MB",
                "retention": "10 days",
                "format": "{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} | {message}",
            }
        ]
    )

def log_error_context(error: Exception, context: Dict[str, Any]) -> None:
    """Log error with additional context.

    Args:
        error: Exception that occurred
        context: Additional context information
    """
    logger.exception(
        "Error occurred: {}\nContext: {}",
        str(error),
        context
    )
```

## Error Handling Patterns

Implement custom exceptions and proper error handling:

```python
class LLMError(Exception):
    """Base exception for LLM-related errors."""
    pass

class ModelNotFoundError(LLMError):
    """Raised when specified model is not available."""
    pass

class TokenLimitError(LLMError):
    """Raised when token limit is exceeded."""
    pass

def handle_llm_request(func: Callable) -> Callable:
    """Decorator for handling LLM API requests.

    Args:
        func: Function to wrap

    Returns:
        Wrapped function with error handling
    """
    @wraps(func)
    async def wrapper(*args: Any, **kwargs: Any) -> Any:
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            logger.exception(f"Error in LLM request: {str(e)}")
            context = {
                "function": func.__name__,
                "args": args,
                "kwargs": kwargs
            }
            log_error_context(e, context)
            raise LLMError(f"LLM request failed: {str(e)}")
    return wrapper
```

## Package Management with UV

Use uv for dependency management. Example configurations:

```toml
# pyproject.toml
[project]
name = "my-llm-project"
version = "0.1.0"
description = "LLM-powered chatbot"
requires-python = ">=3.9"
dependencies = [
    "langchain>=0.1.0",
    "openai>=1.0.0",
    "loguru>=0.7.0",
]

[tool.uv]
python-version = "3.12"
requirements-files = ["requirements.txt"]
```

Common UV commands:
```bash
# Install dependencies
uv add -r requirements.txt

# Add new dependency
uv add langchain

# add dev dependency
uv add --dev pytest

# Update dependencies
uv add --upgrade -r requirements.txt or uv sync --upgrade

# Generate requirements
uv pip freeze > requirements.txt

# See what packages are outdated
uv pip list --outdated
```

## Design Principles

Follow these key principles:

1. DRY (Don't Repeat Yourself):
   - Extract common functionality into reusable components
   - Use inheritance and composition effectively
   - Create utility functions for repeated operations

2. KISS (Keep It Simple, Stupid):
   - Write clear, straightforward code
   - Avoid premature optimization
   - Break complex problems into smaller, manageable pieces

Example of applying DRY and KISS:

```python
from dataclasses import dataclass
from typing import Optional, List

@dataclass
class BasePromptTemplate:
    """Base template for prompt generation.

    Attributes:
        template: Base prompt template
        variables: Required template variables
    """
    template: str
    variables: List[str]

    def format(self, **kwargs: str) -> str:
        """Format template with provided variables.

        Args:
            **kwargs: Template variables

        Returns:
            Formatted prompt

        Raises:
            ValueError: If required variables are missing
        """
        missing = [var for var in self.variables if var not in kwargs]
        if missing:
            raise ValueError(f"Missing required variables: {missing}")
        return self.template.format(**kwargs)

# Example usage - DRY principle in action
qa_template = BasePromptTemplate(
    template="Question: {question}\nContext: {context}\nAnswer:",
    variables=["question", "context"]
)

summary_template = BasePromptTemplate(
    template="Text: {text}\nSummarize:",
    variables=["text"]
)
```

# Part 4 - Design Patterns and LangChain/LangGraph Integration:

```markdown
## Design Patterns for LLM Applications

### Creational Patterns

#### Abstract Factory for Model Creation
```python
from abc import ABC, abstractmethod
from typing import Protocol, Type
from dataclasses import dataclass
from langchain_core.language_models import BaseLLM
from langchain_core.embeddings import Embeddings

class ModelFactory(ABC):
    """Abstract factory for creating LLM-related components."""

    @abstractmethod
    def create_llm(self) -> BaseLLM:
        """Create LLM instance."""
        pass

    @abstractmethod
    def create_embeddings(self) -> Embeddings:
        """Create embeddings model."""
        pass

@dataclass
class OpenAIFactory(ModelFactory):
    """Factory for OpenAI models."""

    api_key: str
    model_name: str = "gpt-3.5-turbo"

    def create_llm(self) -> BaseLLM:
        """Create OpenAI LLM instance."""
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model_name=self.model_name)

    def create_embeddings(self) -> Embeddings:
        """Create OpenAI embeddings model."""
        from langchain_openai import OpenAIEmbeddings
        return OpenAIEmbeddings()

@dataclass
class AnthropicFactory(ModelFactory):
    """Factory for Anthropic models."""

    api_key: str
    model_name: str = "claude-3-opus-20240229"

    def create_llm(self) -> BaseLLM:
        """Create Anthropic LLM instance."""
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model_name=self.model_name)
```

#### Builder Pattern for Chain Construction
```python
from dataclasses import dataclass, field
from typing import List, Optional
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

@dataclass
class ChainBuilder:
    """Builder for constructing LangChain chains."""

    llm: BaseLLM
    prompt_template: Optional[str] = None
    output_parser: Any = field(default_factory=StrOutputParser)
    tools: List[BaseTool] = field(default_factory=list)

    def with_prompt(self, template: str) -> "ChainBuilder":
        """Add prompt template to chain.

        Args:
            template: Prompt template string

        Returns:
            Updated builder instance
        """
        self.prompt_template = template
        return self

    def with_tools(self, tools: List[BaseTool]) -> "ChainBuilder":
        """Add tools to chain.

        Args:
            tools: List of tools to add

        Returns:
            Updated builder instance
        """
        self.tools.extend(tools)
        return self

    def build(self) -> Any:
        """Build the final chain.

        Returns:
            Constructed chain

        Raises:
            ValueError: If required components are missing
        """
        if not self.prompt_template:
            raise ValueError("Prompt template is required")

        prompt = ChatPromptTemplate.from_template(self.prompt_template)
        chain = prompt | self.llm | self.output_parser

        if self.tools:
            from langchain.agents import AgentExecutor, create_react_agent
            agent = create_react_agent(self.llm, self.tools, prompt)
            chain = AgentExecutor(agent=agent, tools=self.tools)

        return chain
```

### Structural Patterns

#### Facade for LangChain Integration
```python
from dataclasses import dataclass
from typing import Any, Dict, List
from langchain_core.messages import BaseMessage

@dataclass
class LangChainFacade:
    """Facade for LangChain operations."""

    model_factory: ModelFactory
    retriever_config: RetrievalConfig

    def __post_init__(self) -> None:
        """Initialize components."""
        self.llm = self.model_factory.create_llm()
        self.embeddings = self.model_factory.create_embeddings()
        self.retriever = self._setup_retriever()

    def _setup_retriever(self) -> Any:
        """Set up document retriever."""
        from langchain_community.vectorstores import Chroma

        db = Chroma(
            embedding_function=self.embeddings,
            persist_directory=str(self.retriever_config.vector_store_path)
        )
        return db.as_retriever()

    async def generate_response(
        self,
        query: str,
        chat_history: List[BaseMessage] = None
    ) -> str:
        """Generate response to user query.

        Args:
            query: User query
            chat_history: Optional chat history

        Returns:
            Generated response
        """
        docs = await self.retriever.aretrieve(query)

        chain = (
            ChainBuilder(self.llm)
            .with_prompt(
                "Context: {context}\nQuestion: {question}\nAnswer:"
            )
            .build()
        )

        response = await chain.ainvoke({
            "context": "\n".join(doc.page_content for doc in docs),
            "question": query
        })

        return response
```

### Behavioral Patterns

#### Strategy Pattern for Different Retrieval Methods
```python
from abc import ABC, abstractmethod
from typing import List, Protocol
from dataclasses import dataclass
from langchain_core.documents import Document

class RetrievalStrategy(Protocol):
    """Protocol for document retrieval strategies."""

    async def retrieve(self, query: str) -> List[Document]:
        """Retrieve relevant documents."""
        ...

@dataclass
class VectorStoreRetrieval(RetrievalStrategy):
    """Vector store-based retrieval strategy."""

    embeddings: Embeddings
    vector_store_path: Path

    async def retrieve(self, query: str) -> List[Document]:
        """Retrieve documents using vector similarity."""
        from langchain_community.vectorstores import Chroma

        db = Chroma(
            embedding_function=self.embeddings,
            persist_directory=str(self.vector_store_path)
        )
        return await db.asimilarity_search(query)

@dataclass
class KeywordRetrieval(RetrievalStrategy):
    """Keyword-based retrieval strategy."""

    documents: List[Document]

    async def retrieve(self, query: str) -> List[Document]:
        """Retrieve documents using keyword matching."""
        from rank_bm25 import BM25Okapi

        corpus = [doc.page_content for doc in self.documents]
        bm25 = BM25Okapi(corpus)
        scores = bm25.get_scores(query.split())

        # Return top 3 documents
        indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]
        return [self.documents[i] for i in indices]
```

### Testing These Patterns

```python
@pytest.mark.asyncio
@pytest.mark.vcr(
    filter_headers=["authorization"],
    match_on=["method", "scheme", "host", "port", "path", "query"]
)
async def test_langchain_facade(
    tmp_path: Path,
    mocker: MockerFixture
) -> None:
    """Test LangChain facade functionality.

    Args:
        tmp_path: Temporary directory
        mocker: Pytest mocker
    """
    # Setup
    config = RetrievalConfig(vector_store_path=tmp_path / "vectors")
    factory = OpenAIFactory(api_key="test-key")
    facade = LangChainFacade(factory, config)

    # Test
    response = await facade.generate_response("What is Python?")
    assert isinstance(response, str)
    assert len(response) > 0

@pytest.mark.asyncio
async def test_retrieval_strategy(tmp_path: Path) -> None:
    """Test different retrieval strategies.

    Args:
        tmp_path: Temporary directory
    """
    embeddings = OpenAIEmbeddings()

    # Test vector store retrieval
    vector_retrieval = VectorStoreRetrieval(
        embeddings=embeddings,
        vector_store_path=tmp_path / "vectors"
    )
    docs = await vector_retrieval.retrieve("test query")
    assert isinstance(docs, list)

    # Test keyword retrieval
    keyword_retrieval = KeywordRetrieval(
        documents=[
            Document(page_content="Python is a programming language"),
            Document(page_content="Python is used for AI")
        ]
    )
    docs = await keyword_retrieval.retrieve("programming language")
    assert len(docs) > 0
```

# Part 5 - Project Structure, Configuration Management, and Final Guidelines:

```markdown
## Project Structure and Configuration

### Directory Structure
```
project_root/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ your_package/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ settings.py
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ models.py
â”‚       â”‚   â””â”€â”€ schemas.py
â”‚       â”œâ”€â”€ llm/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ chains.py
â”‚       â”‚   â”œâ”€â”€ prompts.py
â”‚       â”‚   â””â”€â”€ tools.py
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ helpers.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â””â”€â”€ test_*.py
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ uv.lock
```

### Configuration Management
```python
from dataclasses import dataclass, field
from typing import Optional, Dict, Any
from pathlib import Path
from pydantic_settings import BaseSettings

@dataclass
class AppConfig:
    """Application configuration.

    Attributes:
        env: Environment name
        debug: Debug mode flag
        log_level: Logging level
        log_path: Path to log file
    """
    env: str = "development"
    debug: bool = False
    log_level: str = "INFO"
    log_path: Path = field(default_factory=lambda: Path("logs/app.log"))

@dataclass
class LLMConfig:
    """LLM configuration.

    Attributes:
        provider: LLM provider name
        model_name: Model identifier
        api_key: API key for provider
        temperature: Sampling temperature
    """
    provider: str
    model_name: str
    api_key: str
    temperature: float = 0.7

    @classmethod
    def from_env(cls, settings: "Settings") -> "LLMConfig":
        """Create config from environment settings.

        Args:
            settings: Application settings

        Returns:
            LLM configuration instance
        """
        return cls(
            provider=settings.llm_provider,
            model_name=settings.llm_model_name,
            api_key=settings.llm_api_key,
        )

class Settings(BaseSettings):
    """Application settings from environment variables."""

    # App settings
    app_env: str = "development"
    debug: bool = False

    # LLM settings
    llm_provider: str
    llm_model_name: str
    llm_api_key: str

    # Vector store settings
    vector_store_path: Path = Path("data/vectors")

    class Config:
        """Pydantic config."""
        env_file = ".env"
        env_file_encoding = "utf-8"
```

### UV Package Management
```toml
# pyproject.toml
[project]
name = "your-project"
version = "0.1.0"
description = "LLM-powered application"
requires-python = ">=3.9"
dependencies = [
    "langchain>=0.1.0",
    "langchain-openai>=0.0.2",
    "loguru>=0.7.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
]

[tool.uv]
python-version = "3.9"
requirements-files = ["requirements.txt"]

[tool.uv.scripts]
start = "python -m your_package.main"
test = "pytest tests/"
lint = "ruff check ."
format = "ruff format ."
```

### Testing Configuration
```python
# tests/conftest.py
import pytest
from pathlib import Path
from typing import Generator, Any
from your_package.config.settings import Settings, AppConfig, LLMConfig

@pytest.fixture
def test_settings() -> Generator[Settings, None, None]:
    """Provide test settings.

    Yields:
        Test settings instance
    """
    settings = Settings(
        app_env="test",
        debug=True,
        llm_provider="openai",
        llm_model_name="gpt-3.5-turbo",
        llm_api_key="test-key"
    )
    yield settings

@pytest.fixture
def test_app_config() -> AppConfig:
    """Provide test application config.

    Returns:
        Test app config instance
    """
    return AppConfig(
        env="test",
        debug=True,
        log_level="DEBUG"
    )

@pytest.fixture
def test_llm_config() -> LLMConfig:
    """Provide test LLM config.

    Returns:
        Test LLM config instance
    """
    return LLMConfig(
        provider="openai",
        model_name="gpt-3.5-turbo",
        api_key="test-key",
        temperature=0.5
    )
```

## Final Guidelines

1. Code Organization:
   - Follow the established project structure
   - Keep related functionality together
   - Use clear, descriptive names for files and directories

2. Development Workflow:
   ```bash
   # Setup development environment
   make install

   # Run tests
   uv run pytest tests/

   # Format code
   uv run ruff format .

   # Check linting
   uv run ruff check .
   ```

3. Best Practices:
   - Follow DRY and KISS principles
   - Use type hints consistently
   - Write comprehensive tests
   - Document all public interfaces
   - Use dataclasses for configuration
   - Implement proper error handling
   - Use loguru for logging

4. Discord.py Integration:
   ```python
   import pytest
   import discord.ext.test as dpytest
   from typing import AsyncGenerator

   @pytest.fixture
   async def bot() -> AsyncGenerator[discord.Client, None]:
       """Create test bot instance."""
       bot = discord.Client()
       await bot._async_setup_hook()
       dpytest.configure(bot)
       yield bot
       await dpytest.empty_queue()

   @pytest.mark.discordonly
   async def test_discord_command(bot: discord.Client) -> None:
       """Test Discord command."""
       await dpytest.message("!test")
       assert dpytest.verify().message().content == "Test response"
   ```

5. LangChain/LangGraph Integration:
   - Use the provided design patterns
   - Implement proper testing with VCR
   - Follow the component structure
   - Use proper typing for all components

Remember:
- Keep code simple and readable
- Don't repeat yourself
- Test everything
- Document thoroughly
- Use proper error handling
- Follow established patterns
- Display only differences when using chat to save on tokens.


## Design Patterns

We emphasize the use of design patterns to create robust and maintainable code. Here are some of the patterns we prefer:

- **Composition Over Inheritance**: We favor object composition over class inheritance to avoid subclass explosion and enhance flexibility. This principle is evident in patterns like the Adapter, Bridge, and Decorator patterns.

- **Decorator Pattern**: Useful for dynamically adjusting the behavior of objects without modifying their structure. This pattern is particularly beneficial when subclassing is not feasible.

- **Adapter Pattern**: Allows incompatible interfaces to work together, promoting flexibility and reusability.

- **Global Object Pattern**: Used for creating module-level objects that provide methods for actions, enhancing readability and performance.

For more details on these patterns, refer to the [Python Patterns documentation](#).
