"""
This type stub file was generated by pyright.
"""

import functools
from collections.abc import Callable, Iterable, Sequence
from io import BufferedReader, BytesIO
from typing import NamedTuple, NewType, NoReturn, TYPE_CHECKING, TextIO, Union
from astroid import nodes
from pylint.checkers import BaseRawFileChecker
from pylint.reporters.ureports.nodes import Section
from pylint.typing import MessageDefinitionTuple, Options
from pylint.utils import LinterStats
from pylint.lint import PyLinter

"""
This type stub file was generated by pyright.
"""
if TYPE_CHECKING:
    ...
DEFAULT_MIN_SIMILARITY_LINE = ...
REGEX_FOR_LINES_WITH_CONTENT = ...
Index = NewType("Index", int)
LineNumber = NewType("LineNumber", int)
class LineSpecifs(NamedTuple):
    line_number: LineNumber
    text: str
    ...


HashToIndex_T = dict["LinesChunk", list[Index]]
IndexToLines_T = dict[Index, "SuccessiveLinesLimits"]
STREAM_TYPES = Union[TextIO, BufferedReader, BytesIO]
class CplSuccessiveLinesLimits:
    """Holds a SuccessiveLinesLimits object for each checked file and counts the number
    of common lines between both stripped lines collections extracted from both files.
    """
    __slots__ = ...
    def __init__(self, first_file: SuccessiveLinesLimits, second_file: SuccessiveLinesLimits, effective_cmn_lines_nb: int) -> None:
        ...



CplIndexToCplLines_T = dict["LineSetStartCouple", CplSuccessiveLinesLimits]
class LinesChunk:
    """The LinesChunk object computes and stores the hash of some consecutive stripped
    lines of a lineset.
    """
    __slots__ = ...
    def __init__(self, fileid: str, num_line: int, *lines: Iterable[str]) -> None:
        ...

    def __eq__(self, o: object) -> bool:
        ...

    def __hash__(self) -> int:
        ...

    def __repr__(self) -> str:
        ...

    def __str__(self) -> str:
        ...



class SuccessiveLinesLimits:
    """A class to handle the numbering of begin and end of successive lines.

    :note: Only the end line number can be updated.
    """
    __slots__ = ...
    def __init__(self, start: LineNumber, end: LineNumber) -> None:
        ...

    @property
    def start(self) -> LineNumber:
        ...

    @property
    def end(self) -> LineNumber:
        ...

    @end.setter
    def end(self, value: LineNumber) -> None:
        ...

    def __repr__(self) -> str:
        ...



class LineSetStartCouple(NamedTuple):
    """Indices in both linesets that mark the beginning of successive lines."""
    fst_lineset_index: Index
    snd_lineset_index: Index
    def __repr__(self) -> str:
        ...

    def __eq__(self, other: object) -> bool:
        ...

    def __hash__(self) -> int:
        ...

    def increment(self, value: Index) -> LineSetStartCouple:
        ...



LinesChunkLimits_T = tuple["LineSet", LineNumber, LineNumber]
def hash_lineset(lineset: LineSet, min_common_lines: int = ...) -> tuple[HashToIndex_T, IndexToLines_T]:
    """Return two dicts.

    The first associates the hash of successive stripped lines of a lineset
    to the indices of the starting lines.
    The second dict, associates the index of the starting line in the lineset's stripped lines to the
    couple [start, end] lines number in the corresponding file.

    :param lineset: lineset object (i.e the lines in a file)
    :param min_common_lines: number of successive lines that are used to compute the hash
    :return: a dict linking hashes to corresponding start index and a dict that links this
             index to the start and end lines in the file
    """
    ...

def remove_successive(all_couples: CplIndexToCplLines_T) -> None:
    """Removes all successive entries in the dictionary in argument.

    :param all_couples: collection that has to be cleaned up from successive entries.
                        The keys are couples of indices that mark the beginning of common entries
                        in both linesets. The values have two parts. The first one is the couple
                        of starting and ending line numbers of common successive lines in the first file.
                        The second part is the same for the second file.

    For example consider the following dict:

    >>> all_couples
    {(11, 34): ([5, 9], [27, 31]),
     (23, 79): ([15, 19], [45, 49]),
     (12, 35): ([6, 10], [28, 32])}

    There are two successive keys (11, 34) and (12, 35).
    It means there are two consecutive similar chunks of lines in both files.
    Thus remove last entry and update the last line numbers in the first entry

    >>> remove_successive(all_couples)
    >>> all_couples
    {(11, 34): ([5, 10], [27, 32]),
     (23, 79): ([15, 19], [45, 49])}
    """
    ...

def filter_noncode_lines(ls_1: LineSet, stindex_1: Index, ls_2: LineSet, stindex_2: Index, common_lines_nb: int) -> int:
    """Return the effective number of common lines between lineset1
    and lineset2 filtered from non code lines.

    That is to say the number of common successive stripped
    lines except those that do not contain code (for example
    a line with only an ending parenthesis)

    :param ls_1: first lineset
    :param stindex_1: first lineset starting index
    :param ls_2: second lineset
    :param stindex_2: second lineset starting index
    :param common_lines_nb: number of common successive stripped lines before being filtered from non code lines
    :return: the number of common successive stripped lines that contain code
    """
    ...

class Commonality(NamedTuple):
    cmn_lines_nb: int
    fst_lset: LineSet
    fst_file_start: LineNumber
    fst_file_end: LineNumber
    snd_lset: LineSet
    snd_file_start: LineNumber
    snd_file_end: LineNumber
    ...


class Symilar:
    """Finds copy-pasted lines of code in a project."""
    def __init__(self, min_lines: int = ..., ignore_comments: bool = ..., ignore_docstrings: bool = ..., ignore_imports: bool = ..., ignore_signatures: bool = ...) -> None:
        ...

    def append_stream(self, streamid: str, stream: STREAM_TYPES, encoding: str | None = ...) -> None:
        """Append a file to search for similarities."""
        ...

    def run(self) -> None:
        """Start looking for similarities and display results on stdout."""
        ...

    def get_map_data(self) -> list[LineSet]:
        """Returns the data we can use for a map/reduce process.

        In this case we are returning this instance's Linesets, that is all file
        information that will later be used for vectorisation.
        """
        ...

    def combine_mapreduce_data(self, linesets_collection: list[list[LineSet]]) -> None:
        """Reduces and recombines data into a format that we can report on.

        The partner function of get_map_data()
        """
        ...



def stripped_lines(lines: Iterable[str], ignore_comments: bool, ignore_docstrings: bool, ignore_imports: bool, ignore_signatures: bool, line_enabled_callback: Callable[[str, int], bool] | None = ...) -> list[LineSpecifs]:
    """Return tuples of line/line number/line type with leading/trailing white-space and
    any ignored code features removed.

    :param lines: a collection of lines
    :param ignore_comments: if true, any comment in the lines collection is removed from the result
    :param ignore_docstrings: if true, any line that is a docstring is removed from the result
    :param ignore_imports: if true, any line that is an import is removed from the result
    :param ignore_signatures: if true, any line that is part of a function signature is removed from the result
    :param line_enabled_callback: If called with "R0801" and a line number, a return value of False will disregard
           the line
    :return: the collection of line/line number/line type tuples
    """
    ...

@functools.total_ordering
class LineSet:
    """Holds and indexes all the lines of a single source file.

    Allows for correspondence between real lines of the source file and stripped ones, which
    are the real ones from which undesired patterns have been removed.
    """
    def __init__(self, name: str, lines: list[str], ignore_comments: bool = ..., ignore_docstrings: bool = ..., ignore_imports: bool = ..., ignore_signatures: bool = ..., line_enabled_callback: Callable[[str, int], bool] | None = ...) -> None:
        ...

    def __str__(self) -> str:
        ...

    def __len__(self) -> int:
        ...

    def __getitem__(self, index: int) -> LineSpecifs:
        ...

    def __lt__(self, other: LineSet) -> bool:
        ...

    def __hash__(self) -> int:
        ...

    def __eq__(self, other: object) -> bool:
        ...

    @property
    def stripped_lines(self) -> list[LineSpecifs]:
        ...

    @property
    def real_lines(self) -> list[str]:
        ...



MSGS: dict[str, MessageDefinitionTuple] = ...
def report_similarities(sect: Section, stats: LinterStats, old_stats: LinterStats | None) -> None:
    """Make a layout with some stats about duplication."""
    ...

class SimilaritiesChecker(BaseRawFileChecker, Symilar):
    """Checks for similarities and duplicated code.

    This computation may be memory / CPU intensive, so you
    should disable it if you experience some problems.
    """
    name = ...
    msgs = ...
    MIN_SIMILARITY_HELP = ...
    IGNORE_COMMENTS_HELP = ...
    IGNORE_DOCSTRINGS_HELP = ...
    IGNORE_IMPORTS_HELP = ...
    IGNORE_SIGNATURES_HELP = ...
    options: Options = ...
    reports = ...
    def __init__(self, linter: PyLinter) -> None:
        ...

    def open(self) -> None:
        """Init the checkers: reset linesets and statistics information."""
        ...

    def process_module(self, node: nodes.Module) -> None:
        """Process a module.

        the module's content is accessible via the stream object

        stream must implement the readlines method
        """
        ...

    def close(self) -> None:
        """Compute and display similarities on closing (i.e. end of parsing)."""
        ...

    def get_map_data(self) -> list[LineSet]:
        """Passthru override."""
        ...

    def reduce_map_data(self, linter: PyLinter, data: list[list[LineSet]]) -> None:
        """Reduces and recombines data into a format that we can report on.

        The partner function of get_map_data()

        Calls self.close() to actually calculate and report duplicate code.
        """
        ...



def register(linter: PyLinter) -> None:
    ...

def Run(argv: Sequence[str] | None = ...) -> NoReturn:
    """Standalone command line access point."""
    ...

if __name__ == "__main__":
    ...
