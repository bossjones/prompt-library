"""
This type stub file was generated by pyright.
"""

import click
from click_default_group import DefaultGroup
from llm import Conversation
from typing import Optional

"""
This type stub file was generated by pyright.
"""
DEFAULT_TEMPLATE = ...
class AttachmentType(click.ParamType):
    name = ...
    def convert(self, value, param, ctx):
        ...
    


def attachment_types_callback(ctx, param, values):
    ...

@click.group(cls=DefaultGroup, default="prompt", default_if_no_args=True)
@click.version_option()
def cli():
    """
    Access Large Language Models from the command-line

    Documentation: https://llm.datasette.io/

    LLM can run models from many different providers. Consult the
    plugin directory for a list of available models:

    https://llm.datasette.io/en/stable/plugins/directory.html

    To get started with OpenAI, obtain an API key from them and:

    \b
        $ llm keys set openai
        Enter key: ...

    Then execute a prompt like this:

        llm 'Five outrageous names for a pet pelican'
    """
    ...

@cli.command(name="prompt")
@click.argument("prompt", required=False)
@click.option("-s", "--system", help="System prompt to use")
@click.option("model_id", "-m", "--model", help="Model to use")
@click.option("attachments", "-a", "--attachment", type=AttachmentType(), multiple=True, help="Attachment path or URL or -")
@click.option("attachment_types", "--at", "--attachment-type", type=(str, str), multiple=True, callback=attachment_types_callback, help="Attachment with explicit mimetype")
@click.option("options", "-o", "--option", type=(str, str), multiple=True, help="key/value options for the model")
@click.option("-t", "--template", help="Template to use")
@click.option("-p", "--param", multiple=True, type=(str, str), help="Parameters for template")
@click.option("--no-stream", is_flag=True, help="Do not stream output")
@click.option("-n", "--no-log", is_flag=True, help="Don't log to database")
@click.option("--log", is_flag=True, help="Log prompt and response to the database")
@click.option("_continue", "-c", "--continue", is_flag=True, flag_value=-1, help="Continue the most recent conversation.")
@click.option("conversation_id", "--cid", "--conversation", help="Continue the conversation with the given ID.")
@click.option("--key", help="API key to use")
@click.option("--save", help="Save prompt with this template name")
@click.option("async_", "--async", is_flag=True, help="Run prompt asynchronously")
@click.option("-u", "--usage", is_flag=True, help="Show token usage")
def prompt(prompt, system, model_id, attachments, attachment_types, options, template, param, no_stream, no_log, log, _continue, conversation_id, key, save, async_, usage):
    """
    Execute a prompt

    Documentation: https://llm.datasette.io/en/stable/usage.html

    Examples:

    \b
        llm 'Capital of France?'
        llm 'Capital of France?' -m gpt-4o
        llm 'Capital of France?' -s 'answer in Spanish'

    Multi-modal models can be called with attachments like this:

    \b
        llm 'Extract text from this image' -a image.jpg
        llm 'Describe' -a https://static.simonwillison.net/static/2024/pelicans.jpg
        cat image | llm 'describe image' -a -
        # With an explicit mimetype:
        cat image | llm 'describe image' --at - image/jpeg
    """
    ...

@cli.command()
@click.option("-s", "--system", help="System prompt to use")
@click.option("model_id", "-m", "--model", help="Model to use")
@click.option("_continue", "-c", "--continue", is_flag=True, flag_value=-1, help="Continue the most recent conversation.")
@click.option("conversation_id", "--cid", "--conversation", help="Continue the conversation with the given ID.")
@click.option("-t", "--template", help="Template to use")
@click.option("-p", "--param", multiple=True, type=(str, str), help="Parameters for template")
@click.option("options", "-o", "--option", type=(str, str), multiple=True, help="key/value options for the model")
@click.option("--no-stream", is_flag=True, help="Do not stream output")
@click.option("--key", help="API key to use")
def chat(system, model_id, _continue, conversation_id, template, param, options, no_stream, key):
    """
    Hold an ongoing chat with a model.
    """
    ...

def load_conversation(conversation_id: Optional[str]) -> Optional[Conversation]:
    ...

@cli.group(cls=DefaultGroup, default="list", default_if_no_args=True)
def keys():
    "Manage stored API keys for different models"
    ...

@keys.command(name="list")
def keys_list():
    "List names of all stored keys"
    ...

@keys.command(name="path")
def keys_path_command():
    "Output the path to the keys.json file"
    ...

@keys.command(name="get")
@click.argument("name")
def keys_get(name):
    """
    Return the value of a stored key

    Example usage:

    \b
        export OPENAI_API_KEY=$(llm keys get openai)
    """
    ...

@keys.command(name="set")
@click.argument("name")
@click.option("--value", prompt="Enter key", hide_input=True, help="Value to set")
def keys_set(name, value):
    """
    Save a key in the keys.json file

    Example usage:

    \b
        $ llm keys set openai
        Enter key: ...
    """
    ...

@cli.group(cls=DefaultGroup, default="list", default_if_no_args=True)
def logs():
    "Tools for exploring logged prompts and responses"
    ...

@logs.command(name="path")
def logs_path():
    "Output the path to the logs.db file"
    ...

@logs.command(name="status")
def logs_status():
    "Show current status of database logging"
    ...

@logs.command(name="on")
def logs_turn_on():
    "Turn on logging for all prompts"
    ...

@logs.command(name="off")
def logs_turn_off():
    "Turn off logging for all prompts"
    ...

LOGS_COLUMNS = ...
LOGS_SQL = ...
LOGS_SQL_SEARCH = ...
ATTACHMENTS_SQL = ...
@logs.command(name="list")
@click.option("-n", "--count", type=int, default=None, help="Number of entries to show - defaults to 3, use 0 for all")
@click.option("-p", "--path", type=click.Path(readable=True, exists=True, dir_okay=False), help="Path to log database")
@click.option("-m", "--model", help="Filter by model or model alias")
@click.option("-q", "--query", help="Search for logs matching this string")
@click.option("-t", "--truncate", is_flag=True, help="Truncate long strings in output")
@click.option("-u", "--usage", is_flag=True, help="Include token usage")
@click.option("-r", "--response", is_flag=True, help="Just output the last response")
@click.option("current_conversation", "-c", "--current", is_flag=True, flag_value=-1, help="Show logs from the current conversation")
@click.option("conversation_id", "--cid", "--conversation", help="Show logs for this conversation ID")
@click.option("json_output", "--json", is_flag=True, help="Output logs as JSON")
def logs_list(count, path, model, query, truncate, usage, response, current_conversation, conversation_id, json_output):
    "Show recent logged prompts and their responses"
    ...

@cli.group(cls=DefaultGroup, default="list", default_if_no_args=True)
def models():
    "Manage available models"
    ...

_type_lookup = ...
@models.command(name="list")
@click.option("--options", is_flag=True, help="Show options for each model, if available")
@click.option("async_", "--async", is_flag=True, help="List async models")
def models_list(options, async_):
    "List available models"
    ...

@models.command(name="default")
@click.argument("model", required=False)
def models_default(model):
    "Show or set the default model"
    ...

@cli.group(cls=DefaultGroup, default="list", default_if_no_args=True)
def templates():
    "Manage stored prompt templates"
    ...

@templates.command(name="list")
def templates_list():
    "List available prompt templates"
    ...

@cli.group(cls=DefaultGroup, default="list", default_if_no_args=True)
def aliases():
    "Manage model aliases"
    ...

@aliases.command(name="list")
@click.option("json_", "--json", is_flag=True, help="Output as JSON")
def aliases_list(json_):
    "List current aliases"
    ...

@aliases.command(name="set")
@click.argument("alias")
@click.argument("model_id")
def aliases_set(alias, model_id):
    """
    Set an alias for a model

    Example usage:

    \b
        $ llm aliases set turbo gpt-3.5-turbo
    """
    ...

@aliases.command(name="remove")
@click.argument("alias")
def aliases_remove(alias):
    """
    Remove an alias

    Example usage:

    \b
        $ llm aliases remove turbo
    """
    ...

@aliases.command(name="path")
def aliases_path():
    "Output the path to the aliases.json file"
    ...

@cli.command(name="plugins")
@click.option("--all", help="Include built-in default plugins", is_flag=True)
def plugins_list(all):
    "List installed plugins"
    ...

def display_truncated(text):
    ...

@templates.command(name="show")
@click.argument("name")
def templates_show(name):
    "Show the specified prompt template"
    ...

@templates.command(name="edit")
@click.argument("name")
def templates_edit(name):
    "Edit the specified prompt template using the default $EDITOR"
    ...

@templates.command(name="path")
def templates_path():
    "Output the path to the templates directory"
    ...

@cli.command()
@click.argument("packages", nargs=-1, required=False)
@click.option("-U", "--upgrade", is_flag=True, help="Upgrade packages to latest version")
@click.option("-e", "--editable", help="Install a project in editable mode from this path")
@click.option("--force-reinstall", is_flag=True, help="Reinstall all packages even if they are already up-to-date")
@click.option("--no-cache-dir", is_flag=True, help="Disable the cache")
def install(packages, upgrade, editable, force_reinstall, no_cache_dir):
    """Install packages from PyPI into the same environment as LLM"""
    ...

@cli.command()
@click.argument("packages", nargs=-1, required=True)
@click.option("-y", "--yes", is_flag=True, help="Don't ask for confirmation")
def uninstall(packages, yes):
    """Uninstall Python packages from the LLM environment"""
    ...

@cli.command()
@click.argument("collection", required=False)
@click.argument("id", required=False)
@click.option("-i", "--input", type=click.Path(exists=True, readable=True, allow_dash=True), help="File to embed")
@click.option("-m", "--model", help="Embedding model to use")
@click.option("--store", is_flag=True, help="Store the text itself in the database")
@click.option("-d", "--database", type=click.Path(file_okay=True, allow_dash=False, dir_okay=False, writable=True), envvar="LLM_EMBEDDINGS_DB")
@click.option("-c", "--content", help="Content to embed")
@click.option("--binary", is_flag=True, help="Treat input as binary data")
@click.option("--metadata", help="JSON object metadata to store", callback=_validate_metadata_json)
@click.option("format_", "-f", "--format", type=click.Choice(["json", "blob", "base64", "hex"]), help="Output format")
def embed(collection, id, input, model, store, database, content, binary, metadata, format_):
    """Embed text and store or return the result"""
    ...

@cli.command()
@click.argument("collection")
@click.argument("input_path", type=click.Path(exists=True, dir_okay=False, allow_dash=True, readable=True), required=False)
@click.option("--format", type=click.Choice(["json", "csv", "tsv", "nl"]), help="Format of input file - defaults to auto-detect")
@click.option("--files", type=(click.Path(file_okay=False, dir_okay=True, allow_dash=False), str), multiple=True, help="Embed files in this directory - specify directory and glob pattern")
@click.option("encodings", "--encoding", help="Encoding to use when reading --files", multiple=True)
@click.option("--binary", is_flag=True, help="Treat --files as binary data")
@click.option("--sql", help="Read input using this SQL query")
@click.option("--attach", type=(str, click.Path(file_okay=True, dir_okay=False, allow_dash=False)), multiple=True, help="Additional databases to attach - specify alias and file path")
@click.option("--batch-size", type=int, help="Batch size to use when running embeddings")
@click.option("--prefix", help="Prefix to add to the IDs", default="")
@click.option("-m", "--model", help="Embedding model to use")
@click.option("--store", is_flag=True, help="Store the text itself in the database")
@click.option("-d", "--database", type=click.Path(file_okay=True, allow_dash=False, dir_okay=False, writable=True), envvar="LLM_EMBEDDINGS_DB")
def embed_multi(collection, input_path, format, files, encodings, binary, sql, attach, batch_size, prefix, model, store, database):
    """
    Store embeddings for multiple strings at once

    Input can be CSV, TSV or a JSON list of objects.

    The first column is treated as an ID - all other columns
    are assumed to be text that should be concatenated together
    in order to calculate the embeddings.

    Input data can come from one of three sources:

    \b
    1. A CSV, JSON, TSV or JSON-nl file (including on standard input)
    2. A SQL query against a SQLite database
    3. A directory of files
    """
    ...

@cli.command()
@click.argument("collection")
@click.argument("id", required=False)
@click.option("-i", "--input", type=click.Path(exists=True, readable=True, allow_dash=True), help="File to embed for comparison")
@click.option("-c", "--content", help="Content to embed for comparison")
@click.option("--binary", is_flag=True, help="Treat input as binary data")
@click.option("-n", "--number", type=int, default=10, help="Number of results to return")
@click.option("-d", "--database", type=click.Path(file_okay=True, allow_dash=False, dir_okay=False, writable=True), envvar="LLM_EMBEDDINGS_DB")
def similar(collection, id, input, content, binary, number, database):
    """
    Return top N similar IDs from a collection

    Example usage:

    \b
        llm similar my-collection -c "I like cats"

    Or to find content similar to a specific stored ID:

    \b
        llm similar my-collection 1234
    """
    ...

@cli.group(cls=DefaultGroup, default="list", default_if_no_args=True)
def embed_models():
    "Manage available embedding models"
    ...

@embed_models.command(name="list")
def embed_models_list():
    "List available embedding models"
    ...

@embed_models.command(name="default")
@click.argument("model", required=False)
@click.option("--remove-default", is_flag=True, help="Reset to specifying no default model")
def embed_models_default(model, remove_default):
    "Show or set the default embedding model"
    ...

@cli.group(cls=DefaultGroup, default="list", default_if_no_args=True)
def collections():
    "View and manage collections of embeddings"
    ...

@collections.command(name="path")
def collections_path():
    "Output the path to the embeddings database"
    ...

@collections.command(name="list")
@click.option("-d", "--database", type=click.Path(file_okay=True, allow_dash=False, dir_okay=False, writable=True), envvar="LLM_EMBEDDINGS_DB", help="Path to embeddings database")
@click.option("json_", "--json", is_flag=True, help="Output as JSON")
def embed_db_collections(database, json_):
    "View a list of collections"
    ...

@collections.command(name="delete")
@click.argument("collection")
@click.option("-d", "--database", type=click.Path(file_okay=True, allow_dash=False, dir_okay=False, writable=True), envvar="LLM_EMBEDDINGS_DB", help="Path to embeddings database")
def collections_delete(collection, database):
    """
    Delete the specified collection

    Example usage:

    \b
        llm collections delete my-collection
    """
    ...

def template_dir():
    ...

def logs_db_path():
    ...

def load_template(name):
    ...

def get_history(chat_id):
    ...

def render_errors(errors):
    ...

def logs_on():
    ...

